{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistent output\n",
    "How to make your completions outputs reproducible with the new seed parameter\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/Reproducible_outputs_with_the_seed_parameter.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model level controls for consistent outputs - seed and system_fingerprint\n",
    "import openai\n",
    "import asyncio\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from utils.embeddings_utils import (\n",
    "    get_embedding,\n",
    "    distances_from_embeddings\n",
    ")\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-1106\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_chat_response(\n",
    "    system_message: str, user_request: str, seed: int = None, temperature: float = 0.7\n",
    "):\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_request},\n",
    "        ]\n",
    "\n",
    "        response = openai.chat.completions.create(\n",
    "            model=GPT_MODEL,\n",
    "            messages=messages,\n",
    "            seed=seed,\n",
    "            max_tokens=200,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        response_content = response.choices[0].message.content\n",
    "        system_fingerprint = response.system_fingerprint\n",
    "        prompt_tokens = response.usage.prompt_tokens\n",
    "        completion_tokens = response.usage.total_tokens - response.usage.prompt_tokens\n",
    "\n",
    "        table = f\"\"\"\n",
    "        <table>\n",
    "        <tr><th>Response</th><td>{response_content}</td></tr>\n",
    "        <tr><th>System Fingerprint</th><td>{system_fingerprint}</td></tr>\n",
    "        <tr><th>Number of prompt tokens</th><td>{prompt_tokens}</td></tr>\n",
    "        <tr><th>Number of completion tokens</th><td>{completion_tokens}</td></tr>\n",
    "        </table>\n",
    "        \"\"\"\n",
    "        display(HTML(table))\n",
    "\n",
    "        return response_content\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_average_distance(responses):\n",
    "    \"\"\"\n",
    "    This function calculates the average distance between the embeddings of the responses.\n",
    "    The distance between embeddings is a measure of how similar the responses are.\n",
    "    \"\"\"\n",
    "    # Calculate embeddings for each response\n",
    "    response_embeddings = [get_embedding(response) for response in responses]\n",
    "\n",
    "    # Compute distances between the first response and the rest\n",
    "    distances = distances_from_embeddings(response_embeddings[0], response_embeddings[1:])\n",
    "\n",
    "    # Calculate the average distance\n",
    "    average_distance = sum(distances) / len(distances)\n",
    "\n",
    "    # Return the average distance\n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without the seed parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"a journey to Mars\"\n",
    "system_message = \"You are a helpful assistant.\"\n",
    "user_request = f\"Generate a short excerpt of news about {topic}.\"\n",
    "\n",
    "responses = []\n",
    "\n",
    "\n",
    "async def get_response(i):\n",
    "    print(f'Output {i + 1}\\n{\"-\" * 10}')\n",
    "    response = await get_chat_response(\n",
    "        system_message=system_message, user_request=user_request\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "responses = await asyncio.gather(*[get_response(i) for i in range(5)])\n",
    "average_distance = calculate_average_distance(responses)\n",
    "print(f\"The average similarity between responses is: {average_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `seed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "responses = []\n",
    "\n",
    "async def get_response(i):\n",
    "    print(f'Output {i + 1}\\n{\"-\" * 10}')\n",
    "    response = await get_chat_response(\n",
    "        system_message=system_message,\n",
    "        seed=SEED,\n",
    "        temperature=0,\n",
    "        user_request=user_request,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "responses = await asyncio.gather(*[get_response(i) for i in range(5)])\n",
    "\n",
    "average_distance = calculate_average_distance(responses)\n",
    "print(f\"The average distance between responses is: {average_distance}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
